{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00000: Model validation and optimization\n",
    "Authors: Tobias G. Mueller, Mark A. Buckner\n",
    "Last modified: 4 Dec 2024\n",
    "Contact: __________\n",
    "\n",
    "**Summary**: Here, we use the tiled model predictions as well as validate it against a labeled ground truth.\n",
    "We then calculate the confidence threshold that optimizes f1 score and filter out main model predictions are that level.\n",
    "\n",
    "\n",
    "This script outputs \n",
    "- an optimized model predicting on the whole orthomosaic\n",
    "- model performance metrics\n",
    "\n",
    "The data used in this script was generated in:\n",
    "    `AIggregation/notebooks/02_predict_and_tile.ipynb`\n",
    "    - labelstudio, labeling ground truth of the output from above script\n",
    "\n",
    "Before running this script you must upload the tiled testset output from `02_predict_and_tile.ipynb` into label studio and export the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import scipy\n",
    "import fiftyone as fo\n",
    "\n",
    "# first check the wd is not notebooks but the main folder\n",
    "print(\"cwd is\", os.getcwd())\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"cwd changed to\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import the test subset as fiftyone datasets\n",
    "\n",
    "then merge files with different labels to view them all at once\n",
    "\n",
    "'''\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "gt_directory = \"datasets/testset/groundtruth_testset\" # whereever labelstudio export in yolov5 format\n",
    "tiled_test_directory = \"datasets/testset/tiled_testset\" # target folder for file script\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "# first rename label studio outputs to have same name as fiftyone outputs (label studio adds weird strings upon export)\n",
    "for root, dirs, files in os.walk(gt_directory):\n",
    "    for file in files:\n",
    "        oldname = os.path.join(root,file)\n",
    "        if \"-\" in file:\n",
    "            newname = file.split(\"-\")[1]\n",
    "            os.rename(oldname, os.path.join(root,newname))\n",
    "\n",
    "\n",
    "\n",
    "# import in fiftyone\n",
    "datasettest = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(tiled_test_directory, \"data.yaml\"),\n",
    "    label_field= \"predictions\"\n",
    ")\n",
    "\n",
    "dataset_ground = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(gt_directory,\"data.yaml\"),\n",
    "    label_field= \"ground_truth\"\n",
    ")\n",
    "\n",
    "# merge the datasets, ignoring that they are in different directories and instead merging images/labels with the same file name\n",
    "\n",
    "key_fcn = lambda sample: os.path.basename(sample.filepath)\n",
    "\n",
    "datasettest.merge_samples(dataset_ground, key_fcn=key_fcn)\n",
    "\n",
    "\n",
    "\n",
    "# delete any oblong boxes from the test dataset\n",
    "# these are created by splitting tiles through the middle of an existing bounding box\n",
    "\n",
    "# Computes the dimensions of each bounding box in pixels\n",
    "box_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\n",
    "\n",
    "# get rid of detections where one side of box is greater than 2.5 x the other\n",
    "datasettest_keep=datasettest.select_fields(\"predictions\",\"ground_truth\").filter_labels(\n",
    "    \"predictions\", (box_height < (box_width*2)) & (box_width < (box_height*2)), only_matches=False\n",
    ")\n",
    "\n",
    "# create a subset of just boxes that were removed incase you want to visualize\n",
    "removed_boxes=datasettest.select_fields(\"predictions\").filter_labels(\n",
    "    \"predictions\", (box_height > (box_width*2)) | (box_width > (box_height*2)) \n",
    ")\n",
    "\n",
    "# view that this worked\n",
    "session = fo.launch_app(datasettest_keep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find confidence threshold that gives highest f1 value\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "dataset = datasettest_keep\n",
    "prediction = \"predictions\"\n",
    "gt = \"ground_truth\"\n",
    "lb = .1\n",
    "ub = .99\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to calculate the f1 score of a model\n",
    "def calculate_f1(conf, dataset, prediction, gt):\n",
    "    conf_view = dataset.filter_labels(prediction, F(\"confidence\") >= conf)\n",
    "    results = conf_view.evaluate_detections(prediction,\n",
    "        gt_field= gt,\n",
    "        eval_key=\"eval\",\n",
    "        missing=\"fn\")\n",
    "\n",
    "    fp = sum(conf_view.values(\"eval_fp\"))\n",
    "    tp = sum(conf_view.values(\"eval_tp\"))\n",
    "    fn = sum(conf_view.values(\"eval_fn\"))\n",
    "\n",
    "    f1 = tp/(tp+0.5*(fp+fn))\n",
    "\n",
    "    return -1.0*f1  #make output negative to use fminbound\n",
    "\n",
    "# function find the optimal output of the above function\n",
    "def optimize_conf(lb, ub, dataset, gt, prediction):\n",
    "\n",
    "    res = scipy.optimize.fminbound(\n",
    "                    func=calculate_f1,\n",
    "                    x1=lb,\n",
    "                    x2=ub,\n",
    "                    args=(dataset, prediction, gt),\n",
    "                    xtol=0.01,\n",
    "                    full_output=True\n",
    "    )\n",
    "\n",
    "    best_conf, f1val, ierr, numfunc = res\n",
    "    maxf1 = -1.0*f1val\n",
    "    print(\"\\n \\n     best f1          at confidence\")\n",
    "    print(maxf1, best_conf)\n",
    "    return maxf1, best_conf\n",
    "\n",
    "\n",
    "# save first output, maxf1, to a variable and use it to filder predictions\n",
    "bestconf = optimize_conf(lb=lb, ub=ub, dataset=dataset, gt=gt, prediction=prediction)[1]\n",
    "\n",
    "\n",
    "# evaluate model performance at best conf threshold\n",
    "high_f1_view = datasettest_keep.filter_labels(\"predictions\", F(\"confidence\") > bestconf, only_matches=False)\n",
    "\n",
    "results_f1 = high_f1_view.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"predictions\",\n",
    "    compute_mAP=True\n",
    ")\n",
    "\n",
    "# print performance results\n",
    "print(results_f1.mAP())\n",
    "results_f1.print_report()\n",
    "\n",
    "\n",
    "# create views for false positives and false negatives\n",
    "fp_view = high_f1_view.to_evaluation_patches(eval_key=\"predictions\").match(F(\"type\")==\"fp\").sort_by(\"predictions.detection.confidence\")\n",
    "fn_view = high_f1_view.to_evaluation_patches(eval_key=\"predictions\").match(F(\"type\")==\"fn\").sort_by(\"predictions.detection.confidence\")\n",
    "\n",
    "# view false positives\n",
    "#session.view = fp_view.view()\n",
    "\n",
    "#view false negatives\n",
    "#session.view = fn_view.view()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter full predictions at same confidence threshold and export\n",
    "\n",
    "# reimport full predictions\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "full_prediction_directory = \"datasets/export_predictions/temp\"\n",
    "full_export_directory = \"datasets/export_predictions/final\"\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "# read in full sahi predictions\n",
    "dataset_final = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(full_prediction_directory, \"data.yaml\"),\n",
    "    label_field= \"predictions\"\n",
    ")\n",
    "\n",
    "# filter at optimal confidence\n",
    "dataset_full_f1 = dataset_final.filter_labels(\"predictions\", F(\"confidence\") > bestf1, only_matches=False)\n",
    "\n",
    "# export as final prediction\n",
    "dataset_full_f1.export(\n",
    "        export_dir=full_export_directory,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field=\"prediction\",\n",
    "        include_confidence=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
