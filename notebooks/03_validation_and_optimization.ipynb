{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Model validation and optimization\n",
    "Authors: Tobias G. Mueller, Mark A. Buckner\n",
    "Last modified: 4 Dec 2024\n",
    "Contact: __________\n",
    "\n",
    "**Summary**: Here, we process the tiled model predictions as well as validate the model predictions against a labeled ground truth dataset.\n",
    "We then calculate the confidence threshold that optimizes f1 score and filter out main model predictions at that level.\n",
    "\n",
    "\n",
    "This script outputs \n",
    "- an optimized model predicting on the whole orthomosaic\n",
    "- model performance metrics\n",
    "\n",
    "The data used in this script was generated in:\n",
    "    `AIggregation/notebooks/02_predict_and_tile.ipynb`\n",
    "    - labelstudio, labeling ground truth of the output from above script\n",
    "\n",
    "Before running this script you must have ground truthed the tiled testset output from `02_predict_and_tile.ipynb` using label studio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import scipy\n",
    "import yaml\n",
    "\n",
    "# first check the wd is not notebooks but the main folder\n",
    "print(\"cwd is\", os.getcwd())\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"cwd changed to\", os.getcwd())\n",
    "\n",
    "\n",
    "# set paths\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "gt_directory = \"datasets/testset/groundtruth_testset\"       # Directory with labelstudio groundtruth export extracted. Should contain \"images\" and \"labels\" folders\n",
    "tiled_test_directory = \"datasets/testset/tiled_testset\"     # target folder for tile script (\"target_path\" in previous notebook)\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the tiled predictions and corresponding ground truths that were annotated and remove any split boxes that were cropped during the tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename label studio outputs to have same name as fiftyone outputs (label studio adds unique identifier upon export)\n",
    "for root, dirs, files in os.walk(gt_directory):\n",
    "    for file in files:\n",
    "        oldname = os.path.join(root,file)\n",
    "        if \"-\" in file:\n",
    "            newname = file.split(\"-\")[1]\n",
    "            os.rename(oldname, os.path.join(root,newname))\n",
    "\n",
    "\n",
    "# create a YAML file in the ground_truth folder \n",
    "folder = gt_directory.split('/')[-1] # get end of string (folder name)\n",
    "\n",
    "data = {\n",
    "    'names':{\n",
    "    0: \"nest\"},\n",
    "    'path': os.path.join(\"..\", folder),\n",
    "    'val': \"./images/\"\n",
    "        }\n",
    "with open((os.path.join(gt_directory,\"dataset.yaml\")), 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import test tiles into fiftyone\n",
    "datasettest = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(tiled_test_directory, \"dataset.yaml\"),\n",
    "    label_field= \"prediction\"\n",
    ")\n",
    "\n",
    "# import groundtruthed tiles into fiftyone\n",
    "dataset_ground = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(gt_directory,\"dataset.yaml\"),\n",
    "    label_field= \"ground_truth\"\n",
    ")\n",
    "\n",
    "# merge the datasets, merging images/labels with the same file name\n",
    "key_fcn = lambda sample: os.path.basename(sample.filepath)\n",
    "\n",
    "datasettest.merge_samples(dataset_ground, key_fcn=key_fcn)\n",
    "\n",
    "\n",
    "\n",
    "# delete any oblong box predictions\n",
    "# these are created by splitting tiles through the middle of an existing bounding box. Nest predictions should not be oblong\n",
    "\n",
    "# Compute the dimensions of each bounding box in pixels\n",
    "box_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\n",
    "\n",
    "# remove detections where one side of the box is greater than 2.5 time the other\n",
    "def get_label_fields(sample_collection):\n",
    "    \"\"\"Get the (detection) label fields of a Dataset or DatasetView.\"\"\"\n",
    "    label_fields = list(\n",
    "        sample_collection.get_field_schema(embedded_doc_type=fo.Detections).keys()\n",
    "    )\n",
    "    return label_fields\n",
    "\n",
    "\n",
    "for lf in get_label_fields(datasettest):\n",
    "    datasettest_temp = datasettest.filter_labels(\n",
    "    \"prediction\", (box_height < (box_width*2)) & (box_width < (box_height*2)), only_matches=False)\n",
    "\n",
    "\n",
    "for lf in get_label_fields(datasettest_temp):\n",
    "    datasettest_keep = datasettest_temp.filter_labels(\n",
    "    \"ground_truth\", (box_height < (box_width*2)) & (box_width < (box_height*2)), only_matches=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset view of just boxes that were removed for visualization \n",
    "removed_boxes=datasettest.select_fields(\"prediction\").filter_labels(\n",
    "    \"prediction\", (box_height > (box_width*2)) | (box_width > (box_height*2)) \n",
    ")\n",
    "\n",
    "session = fo.launch_app(removed_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the ground truths to validate our model predictions. \n",
    "\n",
    "Each detection made by the model has a confidence associated with it. We will filter at the confidence threshold that leads to the highest F1 score (the harmonic mean between recall and precision) to get the best overall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "dataset = datasettest_keep      # filtered dataset \n",
    "prediction = \"prediction\"       # prediction name\n",
    "gt = \"ground_truth\"             # ground truth name\n",
    "lb = .1                         # lower confidence to check between\n",
    "ub = .99                        # upper confidence to check between\n",
    "iou = 0.4                       # what IOU to use for FN and FP detections\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "# define function to calculate the f1 score of a model\n",
    "def calculate_f1(conf, dataset, prediction, gt):\n",
    "    conf_view = dataset.filter_labels(prediction, F(\"confidence\") >= conf)\n",
    "    results = conf_view.evaluate_detections(prediction,\n",
    "        gt_field= gt,\n",
    "        eval_key=\"eval\",\n",
    "        missing=\"fn\",\n",
    "        iou = iou)\n",
    "\n",
    "    fp = sum(conf_view.values(\"eval_fp\"))\n",
    "    tp = sum(conf_view.values(\"eval_tp\"))\n",
    "    fn = sum(conf_view.values(\"eval_fn\"))\n",
    "\n",
    "    f1 = tp/(tp+0.5*(fp+fn))\n",
    "\n",
    "    return -1.0*f1  #make output negative to use fminbound in following function\n",
    "\n",
    "calculate_f1(dataset=dataset, conf=.5, prediction=prediction, gt=gt)\n",
    "\n",
    "\n",
    "# define function find the optimal output of the above function\n",
    "def optimize_conf(lb, ub, dataset, gt, prediction):\n",
    "\n",
    "    res = scipy.optimize.fminbound(\n",
    "                    func=calculate_f1,\n",
    "                    x1=lb,\n",
    "                    x2=ub,\n",
    "                    args=(dataset, prediction, gt),\n",
    "                    xtol=0.01,\n",
    "                    full_output=True\n",
    "    )\n",
    "\n",
    "    best_conf, f1val, ierr, numfunc = res\n",
    "    maxf1 = -1.0*f1val\n",
    "    print(\"\\n \\n     best f1          at confidence\")\n",
    "    print(maxf1, best_conf)\n",
    "    return maxf1, best_conf\n",
    "\n",
    "\n",
    "# save second output, best_conf, to a variable for use in filtering predictions\n",
    "bestconf = optimize_conf(lb=lb, ub=ub, dataset=dataset, gt=gt, prediction=prediction)[1]\n",
    "\n",
    "\n",
    "# filter and evaluate model performance at best conf threshold\n",
    "\n",
    "high_f1_view = datasettest_keep.filter_labels(\"prediction\", F(\"confidence\") >= bestconf, only_matches=False)\n",
    "session = fo.launch_app(high_f1_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our test set predictions filtered to optimized F1, we can evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f1 = high_f1_view.evaluate_detections(\n",
    "    prediction,\n",
    "    gt_field=gt,\n",
    "    eval_key=\"prediction\",\n",
    "    missing = \"fn\",\n",
    "    iou = iou,\n",
    "    compute_mAP=True\n",
    ")\n",
    "\n",
    "\n",
    "def eval_model(conf, dataset, prediction, gt):\n",
    "    conf_view = dataset.filter_labels(prediction, F(\"confidence\") >= conf)\n",
    "    results = conf_view.evaluate_detections(prediction,\n",
    "        gt_field= gt,\n",
    "        eval_key=\"eval\",\n",
    "        missing=\"fn\",\n",
    "        iou = iou)\n",
    "\n",
    "    fp = sum(conf_view.values(\"eval_fp\"))\n",
    "    tp = sum(conf_view.values(\"eval_tp\"))\n",
    "    fn = sum(conf_view.values(\"eval_fn\"))\n",
    "\n",
    "    f1 = tp/(tp+0.5*(fp+fn))\n",
    "\n",
    "    print(\"\\n      f1                 precicion           recall\")\n",
    "    return f1, tp/(tp+fp), tp/(tp+fn)\n",
    "\n",
    "print(eval_model(dataset=high_f1_view, conf=bestconf, prediction=prediction, gt=gt))\n",
    "\n",
    "# print performance results\n",
    "print(\"\\n \\n model mAP:\", results_f1.mAP(), \"\\n \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view the false negatives and false positives to get a better understanding of when the model is not accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view false positives\n",
    "\n",
    "fp_view = high_f1_view.to_evaluation_patches(eval_key=\"prediction\").match(F(\"type\")==\"fp\").sort_by(\"prediction.detection.confidence\")\n",
    "\n",
    "session.view = fp_view.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view false negatives\n",
    "\n",
    "fn_view = high_f1_view.to_evaluation_patches(eval_key=\"prediction\").match(F(\"type\")==\"fn\").sort_by(\"prediction.detection.confidence\")\n",
    "\n",
    "session.view = fn_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will filter our full model detections at the optimized confidence threshold and export the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "full_prediction_directory = \"datasets/export_predictions/temp\"      # directory with model detections (created in 02_predict_and_tile.ipynb)\n",
    "full_export_directory = \"datasets/export_predictions/final\"         # directory to output optimized detections\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# read in full sahi predictions\n",
    "dataset_final = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = os.path.join(full_prediction_directory, \"dataset.yaml\"),\n",
    "    label_field= \"prediction\"\n",
    ")\n",
    "\n",
    "# filter at optimal confidence\n",
    "dataset_full_f1 = dataset_final.filter_labels(\"prediction\", F(\"confidence\") > bestconf, only_matches=False)\n",
    "\n",
    "# export as final prediction\n",
    "dataset_full_f1.export(\n",
    "        export_dir=full_export_directory,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field=\"prediction\",\n",
    "        include_confidence=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have optimized nest detections across our whole orthomosaic as well as metrics for how well our model is performing. \n",
    "\n",
    "The model detections can now be georeferenced in `04_georeference.ipynb` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
