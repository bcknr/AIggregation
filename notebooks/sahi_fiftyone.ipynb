{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\obiew\\\\Desktop\\\\github\\\\AIggregation'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first check the wd\n",
    "# this should be AIggregation folder\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "#os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [50.7ms elapsed, 0s remaining, 20.7 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/11/2024 13:04:29 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [50.7ms elapsed, 0s remaining, 20.7 samples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ac5322a2-9824-4885-afce-a19f25a0b375\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14a438f4320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#now import fiftyone\n",
    "import fiftyone as fo\n",
    "#from fiftyone import ViewField as F\n",
    "\n",
    "\n",
    "\n",
    "# Import annotated test image into fiftyone dataset type\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = \"datasets/test_image/test.yaml\"\n",
    ")\n",
    "\n",
    "# open test image instance to verify it loaded correctly\n",
    "session = fo.launch_app(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('model_path is not a valid yolov5 model path: ', AttributeError(\"module 'yolov5' has no attribute 'load'\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\obiew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sahi\\models\\yolov5.py:29\u001b[0m, in \u001b[0;36mYolov5DetectionModel.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43myolov5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model(model)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'yolov5' has no attribute 'load'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# specify AI detection model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m detection_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoDetectionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAIggregation_yolov5m/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#specify path to trained model\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# or 'cuda:0'\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# do a simple yolo prediction without slicing\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# only on the first image in the dataset\u001b[39;00m\n\u001b[0;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m get_prediction(dataset\u001b[38;5;241m.\u001b[39mfirst()\u001b[38;5;241m.\u001b[39mfilepath, detection_model)\n",
      "File \u001b[1;32mc:\\Users\\obiew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sahi\\auto_model.py:68\u001b[0m, in \u001b[0;36mAutoDetectionModel.from_pretrained\u001b[1;34m(model_type, model_path, model, config_path, device, mask_threshold, confidence_threshold, category_mapping, category_remapping, load_at_init, image_size, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m model_class_name \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n\u001b[0;32m     66\u001b[0m DetectionModel \u001b[38;5;241m=\u001b[39m import_model_class(model_type, model_class_name)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDetectionModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_remapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_remapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_at_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_at_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obiew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sahi\\models\\base.py:67\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[1;34m(self, model_path, model, config_path, device, mask_threshold, confidence_threshold, category_mapping, category_remapping, load_at_init, image_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model(model)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obiew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sahi\\models\\yolov5.py:32\u001b[0m, in \u001b[0;36mYolov5DetectionModel.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model(model)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path is not a valid yolov5 model path: \u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('model_path is not a valid yolov5 model path: ', AttributeError(\"module 'yolov5' has no attribute 'load'\"))"
     ]
    }
   ],
   "source": [
    "# import required sahi functions\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "# specify AI detection model\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path='AIggregation_yolov5m/weights/best.pt', #specify path to trained model\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cpu\", # or 'cuda:0'\n",
    ")\n",
    "\n",
    "\n",
    "# do a simple yolo prediction without slicing\n",
    "# only on the first image in the dataset\n",
    "result = get_prediction(dataset.first().filepath, detection_model)\n",
    "\n",
    "\n",
    "# now do sliced prediction with sahi\n",
    "sliced_result = get_sliced_prediction(\n",
    "    dataset.first().filepath,\n",
    "    detection_model,\n",
    "    slice_height = 608,\n",
    "    slice_width = 608,\n",
    "    overlap_height_ratio = 0.1,\n",
    "    overlap_width_ratio = 0.1\n",
    ")\n",
    "\n",
    "\n",
    "# compare number of detections with and without slicing just for fun\n",
    "num_sliced_dets = len(sliced_result.to_fiftyone_detections())\n",
    "num_orig_dets = len(result.to_fiftyone_detections())\n",
    "\n",
    "print(f\"Detections predicted without slicing: {num_orig_dets}\")\n",
    "print(f\"Detections predicted with slicing: {num_sliced_dets}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Now to run and compare different slicing parameters\n",
    "def predict_with_slicing(sample, label_field, **kwargs):\n",
    "    result = get_sliced_prediction(\n",
    "        sample.filepath, detection_model, verbose=0, **kwargs\n",
    "    )\n",
    "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}\n",
    "\n",
    "for sample in dataset.iter_samples(progress=True, autosave=True):\n",
    "    predict_with_slicing(sample, label_field=\"small_slices\", slice_height=304, slice_width=304, **kwargs)\n",
    "    predict_with_slicing(sample, label_field=\"large_slices\", slice_height=608, slice_width=608, **kwargs)\n",
    "\n",
    "\n",
    "#bring up new session to see boxes\n",
    "session.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the different sahi predictions\n",
    "large_slice_results = dataset.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\")\n",
    "small_slice_results = dataset.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\")\n",
    "\n",
    "\n",
    "# print eval results\n",
    "print(\"-\" * 50)\n",
    "print(\"Large slice results:\")\n",
    "large_slice_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small slice results:\")\n",
    "small_slice_results.print_report()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\obiew\\\\Desktop\\\\github\\\\AIggregation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
