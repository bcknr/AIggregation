{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00000: Model prediction and tiling for validation\n",
    "Authors: Tobias G. Mueller, Mark A. Buckner\n",
    "Last modified: 4 Dec 2024\n",
    "Contact: __________\n",
    "\n",
    "**Summary**: Here, we predict on an orthomosaic using out pretrained model. \n",
    "We then split the predictions and image into smaller tiles and take a random 20% for ground truthing and model validation.\n",
    "\n",
    "\n",
    "This script outputs \n",
    "- predicted nest detections in yolov5 format \n",
    "- a random 20% of tiles into a testset folder \n",
    "\n",
    "The data used in this script was generated in:\n",
    "    `AIggregation/notebooks/01_preprocessing.ipynb`\n",
    "\n",
    "This script is followed by `03_validation_and_optimization.ipynb`. However the test tiles created in this script will need to be annotated before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd is /home/tmueller/github/AIggregation/notebooks\n",
      "cwd changed to /home/tmueller/github/AIggregation\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import os\n",
    "import fiftyone as fo\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check the wd is not notebooks but the main folder\n",
    "print(\"cwd is\", os.getcwd())\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"cwd changed to\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect nests on orthomosaic using sahi\n",
    "\n",
    "Using our trained nest detection model we predict on our stitched orthomosaic image using SAHI (slicing aided hyper-inference)   -- `https://github.com/obss/sahi`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [22.5ms elapsed, 0s remaining, 54.6 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:09:29 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [22.5ms elapsed, 0s remaining, 54.6 samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [7.8m elapsed, 0s remaining, 0.0 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:17:16 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [7.8m elapsed, 0s remaining, 0.0 samples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=9945ca31-80f1-4470-bd0a-c4dd873423d5\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fadde7535e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [791.6ms elapsed, 0s remaining, 1.3 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:17:20 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [791.6ms elapsed, 0s remaining, 1.3 samples/s] \n"
     ]
    }
   ],
   "source": [
    "# set paths\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "image_directory = \"datasets/drone_ortho/ortho_clip_23april.png\"     # path to image to be predicted on\n",
    "model_path = \"AIggregation_yolov5m/weights/best.pt\"                 # path to image detection model\n",
    "export_directory = \"datasets/export_predictions/temp\"               # directory to export model predictions to\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "# Import ortho image into a fiftyone dataset \n",
    "dataset_full = fo.Dataset.from_images(\n",
    "    [image_directory]\n",
    ")\n",
    "\n",
    "# specify AI detection model to use for predictions\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=model_path, #specify path to trained model\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cpu\", # \"cpu\" or 'cuda' for GPU\n",
    ")\n",
    "\n",
    "# define function for sliced predictions from sahi\n",
    "def predict_with_slicing(sample, label_field, **kwargs):\n",
    "    result = get_sliced_prediction(\n",
    "        sample.filepath, detection_model, verbose=0, **kwargs\n",
    "    )\n",
    "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())\n",
    "\n",
    "# predict on image, slicing at training image size\n",
    "for sample in dataset_full.iter_samples(progress=True, autosave=True):\n",
    "    predict_with_slicing(sample,\n",
    "                         label_field=\"prediction\",\n",
    "                         slice_height=608, \n",
    "                         slice_width=608,\n",
    "                         overlap_height_ratio = .4, \n",
    "                         overlap_width_ratio=.4\n",
    "    )\n",
    "\n",
    "\n",
    "#launch fiftyone session to see predictions\n",
    "session = fo.launch_app(dataset_full)\n",
    "\n",
    "\n",
    "#export predictions\n",
    "dataset_full.export(\n",
    "        export_dir=export_directory,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field=\"prediction\",\n",
    "        include_confidence=True\n",
    "    )\n",
    "\n",
    "# Importing image, predicting, and export took\n",
    "# 7m 53s using CPU (AMD Ryzen 5 5500 3.6 GHz 6-Core Processor)\n",
    "# 2m 38s using GPU (EVGA SC GAMING GeForce GTX 1060 3GB 3 GB Video Card)\n",
    "# 64 gb DDR4-3200 ram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a test set for validation\n",
    "\n",
    "To assess the model performance we will compare the predicitons vs a labeled random subset.\n",
    "\n",
    "To do this, we split the predicted upon image and its detections into many smaller tiles, then randomly select 20% of them to act as our test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for tiling script\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "source_path = export_directory                      # directory where model prediction exported to\n",
    "target_path = \"./datasets/testset/tiled_testset\"    # directory to save tiled testset\n",
    "img_ext = \".png\"                                   # type of image predicted on\n",
    "tile_size = 608                                     # size of tiles in pixels\n",
    "test_ratio = 0.2                                    # proportion of tiles to keep for testset\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# set max image pixels to none\n",
    "# otherwise pillow thinks large images might be a bomb DOS attack \n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "# run riling script using above parameters\n",
    "%run scripts/yolo_tile_modified.py -source {source_path} -target {target_path} -ext {img_ext}  -size {tile_size} -ratio {test_ratio}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate test set \n",
    "\n",
    "Before continuing to `03_validation_and_optimization.ipynb` the tiled testset must be annotated with ground truth labels.\n",
    "\n",
    "Import the tiled images in the testset folder into labelstudio and annotate them. These will be used in the next notebook to validate the model predictions and optimize the detections. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
