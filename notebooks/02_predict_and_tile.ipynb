{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00000: Model prediction and tiling for validation\n",
    "Authors: Tobias G. Mueller, Mark A. Buckner\n",
    "Last modified: 4 Dec 2024\n",
    "Contact: __________\n",
    "\n",
    "**Summary**: Here, we predict on an orthomosaic using out pretrained model. \n",
    "We then split the predictions and image into smaller tiles and take 20% to\n",
    "be ground truthed and used for model validation.\n",
    "\n",
    "\n",
    "This script outputs \n",
    "- model predictoins in yolov5 format \n",
    "- a random 20% of tiles into a testset folder \n",
    "\n",
    "The data used in this script was generated in:\n",
    "    `AIggregation/notebooks/01_preprocessing.ipynb`\n",
    "\n",
    "Following this script the tiles in the target folder need to be annotated in label studio to create ground truth detections\n",
    "Before `03_validation_and_optimization.ipynb` can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd is /home/tmueller/github/AIggregation/notebooks\n",
      "cwd changed to /home/tmueller/github/AIggregation\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import os\n",
    "import fiftyone as fo\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "# first check the wd is not notebooks but the main folder\n",
    "print(\"cwd is\", os.getcwd())\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"cwd changed to\", os.getcwd())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [22.5ms elapsed, 0s remaining, 54.6 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:09:29 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [22.5ms elapsed, 0s remaining, 54.6 samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [7.8m elapsed, 0s remaining, 0.0 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:17:16 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [7.8m elapsed, 0s remaining, 0.0 samples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=9945ca31-80f1-4470-bd0a-c4dd873423d5\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fadde7535e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 1/1 [791.6ms elapsed, 0s remaining, 1.3 samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2024 09:17:20 - INFO - eta.core.utils -    100% |█████████████████████| 1/1 [791.6ms elapsed, 0s remaining, 1.3 samples/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set paths\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "image_directory = \"datasets/drone_ortho/ortho_clip_23april.png\"\n",
    "model_path = \"AIggregation_yolov5m/weights/best.pt\"\n",
    "export_directory = \"datasets/export_predictions/temp\"\n",
    "# -------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import ortho image as a fiftyone dataset \n",
    "dataset_full = fo.Dataset.from_images(\n",
    "    [image_directory]\n",
    ")\n",
    "\n",
    "# specify AI detection model to use for predictions\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=model_path, #specify path to trained model\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cpu\", # \"cpu\" or 'cuda' for GPU\n",
    ")\n",
    "\n",
    "# function for sliced predictions from sahi\n",
    "def predict_with_slicing(sample, label_field, **kwargs):\n",
    "    result = get_sliced_prediction(\n",
    "        sample.filepath, detection_model, verbose=0, **kwargs\n",
    "    )\n",
    "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())\n",
    "\n",
    "# slice at training image size\n",
    "for sample in dataset_full.iter_samples(progress=True, autosave=True):\n",
    "    predict_with_slicing(sample,\n",
    "                         label_field=\"prediction\",\n",
    "                         slice_height=608, \n",
    "                         slice_width=608,\n",
    "                         overlap_height_ratio = .4, \n",
    "                         overlap_width_ratio=.4\n",
    "    )\n",
    "\n",
    "\n",
    "#launch fiftyone session to see predictions\n",
    "session = fo.launch_app(dataset_full)\n",
    "\n",
    "\n",
    "#export predictions\n",
    "dataset_full.export(\n",
    "        export_dir=export_directory,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field=\"prediction\",\n",
    "        include_confidence=True\n",
    "    )\n",
    "\n",
    "# NEW NUMBERS BELOW FOR SINLGE PREDICITON ARE NEEDED\n",
    "\n",
    "# took 7m 53s for loading image, prediction, and export with CPU (AMD Ryzen 5 5500 3.6 GHz 6-Core Processor)\n",
    "# took 2m 38s on GPU (EVGA SC GAMING GeForce GTX 1060 3GB 3 GB Video Card)\n",
    "# 64 gb DDR4-3200 ram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "To compare the predicitons vs a labeled test set, we tile the exports and ground truth a random 20% of the tiles.\n",
    "\n",
    "The below script takes a folder with yolov5 formated images and labels, tiles them into specified size tiles and saves a percent of random tiles into a new folder. \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# first set max image pixels to none\n",
    "# otherwise pillow thinks its a bomb DOS attack \n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "# run yolo_tile script for each prediction\n",
    "%run scripts/yolo_tile_modified.py -source ./datasets/export_predictions -target ./datasets/testset/tiled_testset -ext .png  -size 608 -ratio 0.2\n",
    "\n",
    "\n",
    "'''\n",
    "    PARAMETERS:\n",
    "\n",
    "    -source         Source folder with images and labels needed to be tiled. \n",
    "    -target         Target folder for a new sliced dataset. Default: \n",
    "    -ext            Image extension in a dataset. Default: .JPG\n",
    "    -size           Size of a tile. Default: 608\n",
    "    -ratio          test split ratio. Default: 0.2 (i.e. 20% of tiles in the test set)\n",
    "'''\n",
    "\n",
    "# the test subset images can now be annotated in labelstudio for groundtruth labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
