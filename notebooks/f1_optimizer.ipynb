{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd is /home/tmueller/github/AIggregation/notebooks\n",
      "cwd changed to /home/tmueller/github/AIggregation\n"
     ]
    }
   ],
   "source": [
    "# first check the wd\n",
    "# this should be AIggregation folder, change if its not\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"cwd is\", os.getcwd())\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"cwd changed to\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 100/100 [255.0ms elapsed, 0s remaining, 394.5 samples/s]     \n",
      " 100% |█████████████████| 165/165 [125.9ms elapsed, 0s remaining, 1.3K samples/s]  \n",
      "Indexing dataset...\n",
      " 100% |█████████████████| 100/100 [50.7ms elapsed, 0s remaining, 2.0K samples/s] \n",
      "Merging samples...\n",
      " 165 [282.9ms elapsed, ? remaining, 586.2 samples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ec759874-26f6-4907-82e3-3ec86305f157\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f703c3a0220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in datasets\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "testdataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = \"datasets/tiled_export/tiles/large_slices_large_overlap/data.yaml\",\n",
    "    label_field= \"large_slices\"\n",
    ")\n",
    "\n",
    "\n",
    "dataset_ground = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    yaml_path = \"datasets/groundtruth_testset/data.yaml\",\n",
    "    label_field= \"ground_truth\"\n",
    ")\n",
    "\n",
    "\n",
    "key_fcn = lambda sample: os.path.basename(sample.filepath)\n",
    "\n",
    "testdataset.merge_samples(dataset_ground, key_fcn=key_fcn)\n",
    "\n",
    "\n",
    "# view that this worked\n",
    "session = fo.launch_app(testdataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████████| 95/95 [296.6ms elapsed, 0s remaining, 320.3 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 90/90 [299.1ms elapsed, 0s remaining, 300.9 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 88/88 [380.8ms elapsed, 0s remaining, 231.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 89/89 [288.9ms elapsed, 0s remaining, 308.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 93/93 [306.5ms elapsed, 0s remaining, 303.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 91/91 [293.1ms elapsed, 0s remaining, 310.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 90/90 [289.0ms elapsed, 0s remaining, 311.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 90/90 [290.1ms elapsed, 0s remaining, 310.2 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 90/90 [288.9ms elapsed, 0s remaining, 311.5 samples/s]      \n",
      "\n",
      " \n",
      "     best f1          at confidence\n",
      "0.7437641723356009 0.6754773552047875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7437641723356009"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import scipy\n",
    "import argparse\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "\n",
    "dataset = testdataset\n",
    "prediction = \"large_slices\"\n",
    "gt = \"ground_truth\"\n",
    "lb = .3\n",
    "ub = .9\n",
    "\n",
    "\n",
    "# define a function to calculate the f1 score of a model\n",
    "def calculate_f1(conf, dataset, prediction, gt):\n",
    "    conf_view = dataset.filter_labels(prediction, F(\"confidence\") >= conf)\n",
    "    results = conf_view.evaluate_detections(prediction,\n",
    "        gt_field= gt,\n",
    "        eval_key=\"eval\",\n",
    "        missing=\"fn\")\n",
    "\n",
    "    fp = sum(conf_view.values(\"eval_fp\"))\n",
    "    tp = sum(conf_view.values(\"eval_tp\"))\n",
    "    fn = sum(conf_view.values(\"eval_fn\"))\n",
    "\n",
    "    f1 = tp/(tp+0.5*(fp+fn))\n",
    "\n",
    "    return -1.0*f1  #make output negative to use fminbound\n",
    "\n",
    "# function to find the confidence that results in best f1 score\n",
    "def optimize_conf(lb, ub, dataset, gt, prediction):\n",
    "\n",
    "    res = scipy.optimize.fminbound(\n",
    "                    func=calculate_f1,\n",
    "                    x1=lb,\n",
    "                    x2=ub,\n",
    "                    args=(dataset, prediction, gt),\n",
    "                    xtol=0.01,\n",
    "                    full_output=True\n",
    "    )\n",
    "\n",
    "    best_conf, f1val, ierr, numfunc = res\n",
    "    maxf1 = -1.0*f1val\n",
    "    print(\"\\n \\n     best f1          at confidence\")\n",
    "    print(maxf1, best_conf)\n",
    "    return maxf1, best_conf\n",
    "\n",
    "\n",
    "# save first output, maxf1, to variable\n",
    "bestf1 = optimize_conf(lb=lb, ub=ub, dataset=dataset, gt=gt, prediction=prediction)[0]\n",
    "\n",
    "bestf1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_conf(lb, ub, gt, prediction):\n",
    "   \n",
    "    best_f1 = -1\n",
    "    best_threshold = None\n",
    "    \n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html\n",
    "\n",
    "    res = scipy.optimize.fminbound(\n",
    "                    func=calculate_f1,\n",
    "                    x1=lb,\n",
    "                    x2=ub,\n",
    "                    args=(prediction,gt),\n",
    "                    xtol=0.01,\n",
    "                    full_output=True\n",
    "    )\n",
    "\n",
    "# the below might need some rescaling\n",
    "    best_conf, f1val, ierr, numfunc = res\n",
    "    return -1.0*f1val, best_conf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# not needed unless I turn this into a script\n",
    "\n",
    "# parse arguments\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-gt\", default=\"ground_truth\", help = \"ground truth detection layer name\")\n",
    "    parser.add_argument(\"-prediction\", default=\"prediction\", help = \"ground truth detection layer name\")\n",
    "    parser.add_argument(\"-dataset\", default=\"dataset\", help = \"ground truth detection layer name\")\n",
    "    parser.add_argument(\"-lb\", default=\".3\", help = \"lower bound of conf to test\")\n",
    "    parser.add_argument(\"-ub\", default=\".9\", help = \"upper bound of conf to test\")\n",
    " \n",
    " \n",
    "\n",
    "    parser.add_argument(\"-ratio\", type=float, default=0.8, help = \"Train/test split ratio. Dafault: 0.8\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "calculate_f1(args.dataset, args.gt, args.prediction, args.lb, args.ub)\n",
    "optimize_conf(args.lb, args.ub, args.gt, args.predition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
